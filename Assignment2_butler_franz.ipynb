{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6cf9e55",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "Melissa Butler and Emma Franz Hughes\n",
    "\\\n",
    "CoSci 5010 AutoML\n",
    "\\\n",
    "April 7, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766e2b21",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The Franz Butler Vineyard (TM) would like to predict how much they can charge for a bottle of wine from their vineyard. The local market has three main price points: Boxed, Good, and Fancy.  Each of these categories contains two sub-categories that can affect pricing, albeit not as significantly. They would like to compare various categorical Machine Learning models with optimized hyperparameters, to decide the most accurate model for predicting the quality of a wine, based on 11 testable features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498ed708",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "Our dataset consists of quantitative descriptions of various wines. There are 1599 wines sampled. We have 11 features for each observation, each with a numeric rating of a physiochemical property of the wine such as alcohol content and acidity. The output variable we seek to predict corresponds to the quality rating of each wine, from sensory data. There are no missing values. A closer inspection of the quality outputs shows a total of 6 integer categories (3-8) as no wine on the list scored a 1, 2, 9, or 10. So, our input values are 11 numerical valued features and our output is a categorical rating ranging from 3-8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c95583",
   "metadata": {},
   "source": [
    "## Experimental Setup \n",
    "We select six estimators implemented in the scikit learn library: a Decision Tree classifier, a Random Forest classifier, an AdaBoost classifier, a K-Nearest Neighbors classifier, a Support Vector classifier, and a Support Vector Regression. The cost function we seek to maximize is accuracy, which was custom coded to compare classifier and regression estimators. For the Support Vector Regression, we define accuracy to be the proportion of predicted values that, once rounded to the nearest integer, match the test values. The parameter space we search includes the above classifiers and a corresponding hyperparameter space for each. We choose hyperparameter ranges based off of documentation from AutoSklearn and the ranges for each hyperparameter can be seen in the code.\n",
    "\n",
    "All features are included and the data is normalized. We use a 3 by 3 nested resampling.  This ensures that each data point is used in both testing and training and prevents overfitting to a single test-train split. We use scikit's KFold to create test-train split indices (after shuffling the data) and for each test-train split obtain optimized hyperparameters for each estimator. We then use the test portion of data to test each estimator with its optimized hyperparameters to obtain an unbiased accuracy estimate.\n",
    "\n",
    "The hyperparameter optimization was performed using BayesSearchCV from scikit-optimize. Thus on each outer fold, we obtained optimized hyperparameters from a Bayesian search with our customized accuracy scoring, a 3-fold cross validation, and 50 iterations for each estimator type. The default surrogate and aqcuisition functions were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b64c2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193b320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1262e3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_assignment1_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e229215",
   "metadata": {},
   "source": [
    "## Results\n",
    "Above are tables for the obtained training and testing accuracy scores for each fold. Also given is a table of the accuracy values for the same estimators using default hyperparameters, copied from Assignment 1. For the hyperparameters obtained for each fold, see the appendix.\n",
    "\n",
    "Regardless of parameter choices, we still see that the Support Vector Regression has a better accuracy than the classifiers we tested. For Fold 1, our best estimator had hyperparameters C=36.07, gamma=6.25, kernel=rbf, and degree=3. For Fold 2, the best estimator had hyperparameters C=1000.0, gamma=0.01, kernel=linear, and degree=4. For Fold 3, the best estimator had hyperparameters C=1000.0, gamma=9.93, kernel=poly, and degree=2. Of course, since the polynomial kernel was not chosen for the first two folds, the degree was ignored. Since our value for C was chosen to be 1000.0 for two of the folds, it is possible our upper bound for C should be increased. However, we did not see an increase in accuracy from the values we obtained in Assignment 1 using the default parameters for Support Vector Regression.\n",
    "\n",
    "We noticed that for the Decision Tree Classifier and Random Forest Classifier our performance after hyperparameter optimization was on average worse than the accuracy in Assignment 1 with the defaul hyperparameters. For the AdaBoost, K Nearest Neighbors, and Support Vector Classifiers we had a slight improvement in accuracy using our optimized hyperparameters over the default hyperparameters. The Support Vector Regression was about the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a420830a",
   "metadata": {},
   "source": [
    "## Resources Used\n",
    "https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/neighbors.html#classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b883fc76",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b8176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from sklearn import preprocessing\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from warnings import filterwarnings \n",
    "filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fa0fdf33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read in data\n",
    "filename = './winequality-red.csv'\n",
    "df = pd.read_csv(filename, delimiter = \";\")\n",
    "X = df.values[:,0:-1]\n",
    "y = df.values[:,-1]\n",
    "\n",
    "#Normalize Feature Data\n",
    "#X = preprocessing.normalize(X, axis = 0)\n",
    "\n",
    "#Print dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "effc1d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_normalize(X):\n",
    "    X_l1 = preprocessing.normalize(X, norm='l1')\n",
    "    X_l2 = preprocessing.normalize(X, norm='l2')\n",
    "    X_max = preprocessing.normalize(X, norm='max')\n",
    "\n",
    "    return X_l1, X_l2, X_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d0e30811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_hpo(X):\n",
    "    # Accuracy scorer\n",
    "    def accuracy(y_pred, y_test):\n",
    "            temp = [abs(y_pred - y_test) < 1] \n",
    "            return sum(sum(temp))/len(y_test)\n",
    "    acc_scorer = make_scorer(accuracy)\n",
    "\n",
    "    num_kfolds = 3 # number of kfolds for both outer and inner loops\n",
    "\n",
    "    # Use scikit learn KFold to create the test-train split indices for the outer sampling, with data shuffled\n",
    "    kf_outer = KFold(n_splits = num_kfolds, shuffle = True) # outer test-train splits\n",
    "    outer_est = [] # outer list of estimators\n",
    "    outer_acc_train = [] # outer list of accuracy scores (from train split)\n",
    "    outer_acc_test = [] # outer list of accuracy scores (from test split)\n",
    "    outer_params = [] #outer list of parameters\n",
    "\n",
    "\n",
    "    # Set estimators and parameters to test\n",
    "    names = [\"Decision\\n Tree\",\n",
    "            \"Random\\n Forest\",\n",
    "            \"AdaBoost\",\n",
    "            \"Knn\"\n",
    "            #\"SVM\",\n",
    "            #\"SVR\"\n",
    "            ]\n",
    "\n",
    "    estimators = [tree.DecisionTreeClassifier(),\n",
    "                RandomForestClassifier(),\n",
    "                AdaBoostClassifier(),\n",
    "                KNeighborsClassifier()\n",
    "                #SVC(),\n",
    "                #SVR()\n",
    "                ]\n",
    "\n",
    "    bayes_param_spaces = [{\"max_depth\": Integer(6, 20), # values of max_depth are integers from 6 to 20\n",
    "            \"max_features\": Categorical(['sqrt','log2']), \n",
    "            \"min_samples_leaf\": Integer(2, 10),\n",
    "            \"min_samples_split\": Integer(2, 10)\n",
    "        }, # Decision tree search space\n",
    "        {\"bootstrap\": Categorical([True, False]), # values for boostrap can be either True or False\n",
    "            \"max_depth\": Integer(6, 20),\n",
    "            \"max_features\": Categorical(['sqrt','log2']), \n",
    "            \"min_samples_leaf\": Integer(2, 10),\n",
    "            \"min_samples_split\": Integer(2, 10),\n",
    "            \"n_estimators\": Integer(100, 500)\n",
    "        }, # Random forest search space\n",
    "        {\"n_estimators\": Integer(1e1, 1e3),\n",
    "            \"learning_rate\": Real(1e-2,10.0)\n",
    "        }, #AdaBoost search space\n",
    "        {\"n_neighbors\": Integer(1,10),\n",
    "            \"weights\": Categorical(['uniform', 'distance']),\n",
    "            \"algorithm\": Categorical(['auto','ball_tree', 'kd_tree', 'brute']),\n",
    "        }, #KNN search space\n",
    "        {\"kernel\": Categorical(['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "            \"C\": Real(1e-2, 1e3, prior = \"log-uniform\"),\n",
    "            \"gamma\": Real(1e-2,1e3, prior = \"log-uniform\"),\n",
    "            \"degree\": Integer(2,7)\n",
    "        }, #SVC search space\n",
    "        {\"kernel\": Categorical(['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "            \"C\": Real(1e-2, 1e3, prior = \"log-uniform\"),\n",
    "            \"gamma\": Real(1e-2,1e3, prior = \"log-uniform\"),\n",
    "            \"degree\": Integer(2,7)\n",
    "        } #SVM search space\n",
    "            ]\n",
    "        \n",
    "    random_param_spaces = [{'max_depth':(6, 20), # values of max_depth are integers from 6 to 20\n",
    "            'max_features': ['sqrt','log2'], \n",
    "            'min_samples_leaf': (2, 10),\n",
    "            'min_samples_split': (2, 10)\n",
    "        }, # Decision tree search space\n",
    "        {'bootstrap': [True, False], # values for boostrap can be either True or False\n",
    "            'max_depth': (6, 20),\n",
    "            'max_features': ['sqrt','log2'], \n",
    "            'min_samples_leaf': (2, 10),\n",
    "            'min_samples_split': (2, 10),\n",
    "            'n_estimators': (100, 500)\n",
    "        }, # Random forest search space\n",
    "        {'n_estimators': (1, 100),\n",
    "            'learning_rate': (1e-2,10.0)\n",
    "        }, #AdaBoost search space\n",
    "        {'n_neighbors': (1,10),\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'algorithm': ['auto','ball_tree', 'kd_tree', 'brute'],\n",
    "        }] #KNN search space\n",
    "    \"\"\"\n",
    "        {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'C': (1e-2, 1e3, prior = \"log-uniform\"),\n",
    "            'gamma': (1e-2,1e3, prior = \"log-uniform\"),\n",
    "            'degree': (2,7)\n",
    "        }, #SVC search space\n",
    "        {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'C': (1e-2, 1e3, prior = \"log-uniform\"),\n",
    "            'gamma': (1e-2,1e3, prior = \"log-uniform\"),\n",
    "            'degree': (2,7)\n",
    "        } #SVM search space\n",
    "            ]\n",
    "    \"\"\"\n",
    "    for i, (train_index, test_index) in enumerate(kf_outer.split(X)):\n",
    "        best_est = []\n",
    "        best_acc_train = []\n",
    "        best_acc_test = []\n",
    "        best_params = []\n",
    "        X_train = X[train_index, :]\n",
    "        X_test = X[test_index, :]\n",
    "        y_train = y[train_index]\n",
    "        y_test = y[test_index]\n",
    "        kf_inner = KFold(n_splits = num_kfolds, shuffle = True)\n",
    "        \n",
    "        # Use a random search for hyperparameters for each estimator\n",
    "        for j in range(len(estimators)):\n",
    "            classifier = estimators[j]\n",
    "            param_space = random_param_spaces[j]\n",
    "            random_search = RandomizedSearchCV(classifier, param_space, cv=kf_inner,\n",
    "                                            n_iter=1,\n",
    "                                            scoring=acc_scorer,\n",
    "                                            verbose=False,\n",
    "                                            n_jobs=-1)\n",
    "            random_search.fit(X_train, y_train)\n",
    "            best_est.append(random_search.best_estimator_)\n",
    "            best_acc_train.append(random_search.best_score_)\n",
    "            best_params.append(random_search.best_params_)\n",
    "            estimator_test = random_search.best_estimator_.fit(X_train, y_train)\n",
    "            y_pred = estimator_test.predict(X_test)\n",
    "            acc = accuracy(y_pred, y_test)\n",
    "            best_acc_test.append(acc)\n",
    "\n",
    "        outer_est.append(best_est)\n",
    "        outer_acc_train.append(best_acc_train)\n",
    "        outer_acc_test.append(best_acc_test)\n",
    "        outer_params.append(best_params)\n",
    "\n",
    "        table_train_acc = pd.DataFrame(data = outer_acc_train, index =['Fold 1', 'Fold 2', 'Fold 3'], columns = names2)\n",
    "        table_test_acc = pd.DataFrame(data = outer_acc_test, index =['Fold 1', 'Fold 2', 'Fold 3'], columns = names2)\n",
    "\n",
    "        return table_train_acc, table_test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f21d2ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----l1-----\n",
      "        Decision Tree  Random Forest  AdaBoost       Knn\n",
      "Fold 1       0.515907       0.581606  0.498162  0.469977\n",
      "Fold 2       0.515907       0.581606  0.498162  0.469977\n",
      "Fold 3       0.515907       0.581606  0.498162  0.469977\n",
      "-----l2-----\n",
      "        Decision Tree  Random Forest  AdaBoost       Knn\n",
      "Fold 1       0.496236       0.585367  0.473722  0.501859\n",
      "Fold 2       0.496236       0.585367  0.473722  0.501859\n",
      "Fold 3       0.496236       0.585367  0.473722  0.501859\n",
      "-----l3-----\n",
      "        Decision Tree  Random Forest  AdaBoost       Knn\n",
      "Fold 1       0.496241       0.560046  0.485931  0.493435\n",
      "Fold 2       0.496241       0.560046  0.485931  0.493435\n",
      "Fold 3       0.496241       0.560046  0.485931  0.493435\n"
     ]
    }
   ],
   "source": [
    "X_list = my_normalize(X)\n",
    "norm_names = ['l1','l2','l3']\n",
    "for n in range(len(X_list)):\n",
    "    print('-----'+norm_names[n]+'-----')\n",
    "    X = X_list[n]\n",
    "    train_df, test_df = my_hpo(X_list[n])\n",
    "    print(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f60899",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def my_table():\n",
    "    # Create table of accuracy for each estimator for each fold\n",
    "    #acc_means = np.average(acc, axis = 1)\n",
    "    names2 = [\"Decision Tree\",\n",
    "            \"Random Forest\",\n",
    "            \"AdaBoost\",\n",
    "            \"Knn\"\n",
    "            #\"SVM\",\n",
    "            #\"SVR\"\n",
    "            ]\n",
    "    #means = np.column_stack((acc_means, acc_means_threecat))\n",
    "    table_train_acc = pd.DataFrame(data = outer_acc_train, index =['Fold 1', 'Fold 2', 'Fold 3'], columns = names2)\n",
    "    table_test_acc = pd.DataFrame(data = outer_acc_test, index =['Fold 1', 'Fold 2', 'Fold 3'], columns = names2)\n",
    "    assignment1_data = [0.625366, 0.697952, 0.524104, 0.567897]\n",
    "    #, 0.579721, 0.888671]\n",
    "    table_assignment1_acc = pd.DataFrame(data = assignment1_data, columns =['Default hyperparameters accuracy'], index = names2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd7b047",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d6e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "99c8772be7a42d3e7dfb23e3f09493b2460db951f4a8d78f271dcc6a026d4283"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
